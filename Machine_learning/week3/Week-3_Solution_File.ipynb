{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be trying to understand sentiment of tweets about the company Apple, By using the twitter for better understand public perception, Apple wants to monitor how people feel over time and how people receive new announcements.\n",
    "\n",
    "Our challenge is to see if we can correctly classify tweets as being negative, positive, or neither about Apple.\n",
    "\n",
    "Sentiment Mining - Apple\n",
    "•\tApple is a computer company known for its laptops, phones, tablets, and personal media players\n",
    "•\tLarge numbers of fans, large number of “haters”\n",
    "•\tApple wants to monitor how people feel about them over time, and how people receive new announcements.\n",
    "\n",
    "Problem Statement:\n",
    "•\tCan we correctly classify tweets as being negative, positive, or neither about Apple?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:33.527545Z",
     "start_time": "2020-04-16T13:31:32.032505Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re\n",
    "import nltk \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the csv file available in the working or specified directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:33.537515Z",
     "start_time": "2020-04-16T13:31:33.528504Z"
    }
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.chdir('C:\\\\GL Class\\DSBA - Python\\Machine Learning\\Week 3')\n",
    "Apple_tweets = pd.read_csv(\"Apple_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:33.756926Z",
     "start_time": "2020-04-16T13:31:33.539487Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have to say, Apple has by far the best custo...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iOS 7 is so fricking smooth &amp; beautiful!! #Tha...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOVE U @APPLE</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thank you @apple, loving my new iPhone 5S!!!!!...</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.@apple has the best customer service. In and ...</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Avg\n",
       "0  I have to say, Apple has by far the best custo...  2.0\n",
       "1  iOS 7 is so fricking smooth & beautiful!! #Tha...  2.0\n",
       "2                                      LOVE U @APPLE  1.8\n",
       "3  Thank you @apple, loving my new iPhone 5S!!!!!...  1.8\n",
       "4  .@apple has the best customer service. In and ...  1.8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Apple_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Exploration in Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To create a temporary function lambda can be used. These functions do not require a name like a def function, however the output is same as defining a permanent function**\n",
    "**As these function are temporary, memory comsumption is less in comparison to permanent function. Also there are multiple ways to get a similar output**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:33.874611Z",
     "start_time": "2020-04-16T13:31:33.758921Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>totalwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have to say, Apple has by far the best custo...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iOS 7 is so fricking smooth &amp; beautiful!! #Tha...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOVE U @APPLE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thank you @apple, loving my new iPhone 5S!!!!!...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.@apple has the best customer service. In and ...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  totalwords\n",
       "0  I have to say, Apple has by far the best custo...          19\n",
       "1  iOS 7 is so fricking smooth & beautiful!! #Tha...          10\n",
       "2                                      LOVE U @APPLE           3\n",
       "3  Thank you @apple, loving my new iPhone 5S!!!!!...          11\n",
       "4  .@apple has the best customer service. In and ...          16"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's get a word count without writing a lambda function\n",
    "\n",
    "Apple_tweets['totalwords'] = [len(x.split()) for x in Apple_tweets['Tweet'].tolist()]\n",
    "Apple_tweets[['Tweet','totalwords']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:34.038179Z",
     "start_time": "2020-04-16T13:31:33.876573Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have to say, Apple has by far the best custo...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iOS 7 is so fricking smooth &amp; beautiful!! #Tha...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOVE U @APPLE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thank you @apple, loving my new iPhone 5S!!!!!...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.@apple has the best customer service. In and ...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  word_count\n",
       "0  I have to say, Apple has by far the best custo...          19\n",
       "1  iOS 7 is so fricking smooth & beautiful!! #Tha...          10\n",
       "2                                      LOVE U @APPLE           3\n",
       "3  Thank you @apple, loving my new iPhone 5S!!!!!...          12\n",
       "4  .@apple has the best customer service. In and ...          16"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Apple_tweets['word_count'] = Apple_tweets['Tweet'].apply(lambda x: len(str(x).split(\" \")))\n",
    "Apple_tweets[['Tweet','word_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Characters- including spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:34.155826Z",
     "start_time": "2020-04-16T13:31:34.040168Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have to say, Apple has by far the best custo...</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iOS 7 is so fricking smooth &amp; beautiful!! #Tha...</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOVE U @APPLE</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thank you @apple, loving my new iPhone 5S!!!!!...</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.@apple has the best customer service. In and ...</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  char_count\n",
       "0  I have to say, Apple has by far the best custo...         101\n",
       "1  iOS 7 is so fricking smooth & beautiful!! #Tha...          60\n",
       "2                                      LOVE U @APPLE          13\n",
       "3  Thank you @apple, loving my new iPhone 5S!!!!!...          91\n",
       "4  .@apple has the best customer service. In and ...          82"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Apple_tweets['char_count'] = Apple_tweets['Tweet'].str.len() ## this also includes spaces\n",
    "Apple_tweets[['Tweet','char_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:34.266564Z",
     "start_time": "2020-04-16T13:31:34.156823Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>avg_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have to say, Apple has by far the best custo...</td>\n",
       "      <td>4.368421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iOS 7 is so fricking smooth &amp; beautiful!! #Tha...</td>\n",
       "      <td>5.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOVE U @APPLE</td>\n",
       "      <td>3.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thank you @apple, loving my new iPhone 5S!!!!!...</td>\n",
       "      <td>7.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.@apple has the best customer service. In and ...</td>\n",
       "      <td>4.187500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  avg_word\n",
       "0  I have to say, Apple has by far the best custo...  4.368421\n",
       "1  iOS 7 is so fricking smooth & beautiful!! #Tha...  5.100000\n",
       "2                                      LOVE U @APPLE  3.666667\n",
       "3  Thank you @apple, loving my new iPhone 5S!!!!!...  7.272727\n",
       "4  .@apple has the best customer service. In and ...  4.187500"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "Apple_tweets['avg_word'] = Apple_tweets['Tweet'].apply(lambda x: avg_word(x))\n",
    "Apple_tweets[['Tweet','avg_word']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:34.498950Z",
     "start_time": "2020-04-16T13:31:34.269522Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:34.662507Z",
     "start_time": "2020-04-16T13:31:34.499905Z"
    }
   },
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:34.847976Z",
     "start_time": "2020-04-16T13:31:34.663501Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\v2n/nltk_data'\n    - 'C:\\\\Users\\\\v2n\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\v2n\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\v2n\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\v2n\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\v2n/nltk_data'\n    - 'C:\\\\Users\\\\v2n\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\v2n\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\v2n\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\v2n\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-8dbeb967950f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mstop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mApple_tweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stopwords'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mApple_tweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mApple_tweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Tweet'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'stopwords'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\v2n/nltk_data'\n    - 'C:\\\\Users\\\\v2n\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\v2n\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\v2n\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\v2n\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "Apple_tweets['stopwords'] = Apple_tweets['Tweet'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "Apple_tweets[['Tweet','stopwords']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of special character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:34.919782Z",
     "start_time": "2020-04-16T13:31:34.848973Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>hastags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have to say, Apple has by far the best custo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iOS 7 is so fricking smooth &amp; beautiful!! #Tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOVE U @APPLE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thank you @apple, loving my new iPhone 5S!!!!!...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.@apple has the best customer service. In and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  hastags\n",
       "0  I have to say, Apple has by far the best custo...        0\n",
       "1  iOS 7 is so fricking smooth & beautiful!! #Tha...        1\n",
       "2                                      LOVE U @APPLE        0\n",
       "3  Thank you @apple, loving my new iPhone 5S!!!!!...        2\n",
       "4  .@apple has the best customer service. In and ...        0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Apple_tweets['hastags'] = Apple_tweets['Tweet'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "Apple_tweets[['Tweet','hastags']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>hastags_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have to say, Apple has by far the best custo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iOS 7 is so fricking smooth &amp; beautiful!! #Tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOVE U @APPLE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thank you @apple, loving my new iPhone 5S!!!!!...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.@apple has the best customer service. In and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  hastags_1\n",
       "0  I have to say, Apple has by far the best custo...          0\n",
       "1  iOS 7 is so fricking smooth & beautiful!! #Tha...          1\n",
       "2                                      LOVE U @APPLE          0\n",
       "3  Thank you @apple, loving my new iPhone 5S!!!!!...          2\n",
       "4  .@apple has the best customer service. In and ...          0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Apple_tweets['hastags_1'] = Apple_tweets['Tweet'].apply(lambda tweet: len([token for token in tweet.split() if token.startswith('#')]))\n",
    "Apple_tweets[['Tweet','hastags_1']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:35.060445Z",
     "start_time": "2020-04-16T13:31:34.921777Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>numerics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have to say, Apple has by far the best custo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iOS 7 is so fricking smooth &amp; beautiful!! #Tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOVE U @APPLE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thank you @apple, loving my new iPhone 5S!!!!!...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.@apple has the best customer service. In and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  numerics\n",
       "0  I have to say, Apple has by far the best custo...         0\n",
       "1  iOS 7 is so fricking smooth & beautiful!! #Tha...         1\n",
       "2                                      LOVE U @APPLE         0\n",
       "3  Thank you @apple, loving my new iPhone 5S!!!!!...         0\n",
       "4  .@apple has the best customer service. In and ...         0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Apple_tweets['numerics'] = Apple_tweets['Tweet'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "Apple_tweets[['Tweet','numerics']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Uppercase Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:35.177127Z",
     "start_time": "2020-04-16T13:31:35.061403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have to say, Apple has by far the best custo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iOS 7 is so fricking smooth &amp; beautiful!! #Tha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOVE U @APPLE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thank you @apple, loving my new iPhone 5S!!!!!...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.@apple has the best customer service. In and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  upper\n",
       "0  I have to say, Apple has by far the best custo...      2\n",
       "1  iOS 7 is so fricking smooth & beautiful!! #Tha...      0\n",
       "2                                      LOVE U @APPLE      3\n",
       "3  Thank you @apple, loving my new iPhone 5S!!!!!...      1\n",
       "4  .@apple has the best customer service. In and ...      0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Apple_tweets['upper'] = Apple_tweets['Tweet'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "Apple_tweets[['Tweet','upper']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower Case conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:35.271841Z",
     "start_time": "2020-04-16T13:31:35.178091Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    i have to say, apple has by far the best custo...\n",
       "1    ios 7 is so fricking smooth & beautiful!! #tha...\n",
       "2                                        love u @apple\n",
       "3    thank you @apple, loving my new iphone 5s!!!!!...\n",
       "4    .@apple has the best customer service. in and ...\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Apple_tweets['Tweet'] = Apple_tweets['Tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "Apple_tweets['Tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:35.468354Z",
     "start_time": "2020-04-16T13:31:35.272838Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    i have to say apple has by far the best custom...\n",
       "1    ios 7 is so fricking smooth  beautiful thanxap...\n",
       "2                                         love u apple\n",
       "3    thank you apple loving my new iphone 5s apple ...\n",
       "4    apple has the best customer service in and out...\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Apple_tweets['Tweet'] = Apple_tweets['Tweet'].str.replace('[^\\w\\s]','')\n",
    "Apple_tweets['Tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:35.634907Z",
     "start_time": "2020-04-16T13:31:35.469312Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\v2n/nltk_data'\n    - 'C:\\\\Users\\\\v2n\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\v2n\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\v2n\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\v2n\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\v2n/nltk_data'\n    - 'C:\\\\Users\\\\v2n\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\v2n\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\v2n\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\v2n\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-6c3c78b77e9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mstop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mApple_tweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Tweet'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mApple_tweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mApple_tweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Tweet'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\v2n/nltk_data'\n    - 'C:\\\\Users\\\\v2n\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\v2n\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\v2n\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\v2n\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "Apple_tweets['Tweet'] = Apple_tweets['Tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "Apple_tweets['Tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Words Removal\n",
    "1. **We will create a list of 10 frequently occuring words and then decide if we need to remove it or retain it.**\n",
    "2. **Reason is that this file has tweets related to Apple.. So no point in keeping the word like Apple, unless we have tweets from other brands**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:35.748567Z",
     "start_time": "2020-04-16T13:31:35.636864Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apple     1297\n",
       "the        489\n",
       "to         326\n",
       "iphone     257\n",
       "i          251\n",
       "a          238\n",
       "is         209\n",
       "you        187\n",
       "and        183\n",
       "for        145\n",
       "in         143\n",
       "my         142\n",
       "of         137\n",
       "it         137\n",
       "new        113\n",
       "on         107\n",
       "with       104\n",
       "that        92\n",
       "5s          91\n",
       "ipad        88\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(Apple_tweets['Tweet']).split()).value_counts()[:20]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:35.871105Z",
     "start_time": "2020-04-16T13:31:35.749563Z"
    }
   },
   "outputs": [],
   "source": [
    "freq =['apple','get']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **As we are talking about multiple products hence iphone will be kept, similarly some tweets do relate to old products without mentioning the word old, hence even new would be kept in the tweets.**\n",
    "2. **hence only apple and get would be removed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:36.009696Z",
     "start_time": "2020-04-16T13:31:35.872064Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    i have to say has by far the best customer car...\n",
       "1     ios 7 is so fricking smooth beautiful thanxapple\n",
       "2                                               love u\n",
       "3    thank you loving my new iphone 5s iphone5s pic...\n",
       "4    has the best customer service in and out with ...\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Apple_tweets['Tweet'] = Apple_tweets['Tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "Apple_tweets['Tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rare Words Removal\n",
    "**This is done as association of these less occurring words with the existing words could be a noise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:36.118405Z",
     "start_time": "2020-04-16T13:31:36.011690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "glam              1\n",
       "takentooz         1\n",
       "outchea           1\n",
       "hella             1\n",
       "compre            1\n",
       "sues              1\n",
       "eligible          1\n",
       "racebaiting       1\n",
       "brenberryblast    1\n",
       "purchases         1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(Apple_tweets['Tweet']).split()).value_counts()[-10:]\n",
    "freq\n",
    "## As it is difficult to make out if these words will have association in text analytics or not, \n",
    "## hence to start with these words are kept in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming -refers to the removal of suffices, like “ing”, “ly”, “s”, etc. by a simple rule-based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:36.253048Z",
     "start_time": "2020-04-16T13:31:36.121398Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    i have to say ha by far the best custom care s...\n",
       "1             io 7 is so frick smooth beauti thanxappl\n",
       "2                                               love u\n",
       "3    thank you love my new iphon 5s iphone5 pictwit...\n",
       "4    ha the best custom servic in and out with a ne...\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "st = PorterStemmer()\n",
    "Apple_tweets['Tweet'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:36.377750Z",
     "start_time": "2020-04-16T13:31:36.256038Z"
    }
   },
   "outputs": [],
   "source": [
    "def Tweet(x):\n",
    "    if x >= 0:\n",
    "        return \"Positive\"\n",
    "    else: return \"Negative\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T10:52:41.105760Z",
     "start_time": "2020-04-10T10:52:41.100002Z"
    }
   },
   "source": [
    "### Now to get the sentiments as positive and negative , convert the Avg column . If value is >= 0  then tweet is Positive, else tweet is Negative. This will make a dependent variable as a binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:36.482465Z",
     "start_time": "2020-04-16T13:31:36.381701Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Avg</th>\n",
       "      <th>totalwords</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>hastags</th>\n",
       "      <th>hastags_1</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i have to say has by far the best customer car...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>101</td>\n",
       "      <td>4.368421</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ios 7 is so fricking smooth beautiful thanxapple</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>60</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love u</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thank you loving my new iphone 5s iphone5s pic...</td>\n",
       "      <td>1.8</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>91</td>\n",
       "      <td>7.272727</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>has the best customer service in and out with ...</td>\n",
       "      <td>1.8</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>82</td>\n",
       "      <td>4.187500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Avg  totalwords  \\\n",
       "0  i have to say has by far the best customer car...  2.0          19   \n",
       "1   ios 7 is so fricking smooth beautiful thanxapple  2.0          10   \n",
       "2                                             love u  1.8           3   \n",
       "3  thank you loving my new iphone 5s iphone5s pic...  1.8          11   \n",
       "4  has the best customer service in and out with ...  1.8          16   \n",
       "\n",
       "   word_count  char_count  avg_word  hastags  hastags_1  numerics  upper  \\\n",
       "0          19         101  4.368421        0          0         0      2   \n",
       "1          10          60  5.100000        1          1         1      0   \n",
       "2           3          13  3.666667        0          0         0      3   \n",
       "3          12          91  7.272727        2          2         0      1   \n",
       "4          16          82  4.187500        0          0         0      0   \n",
       "\n",
       "  Sentiment  \n",
       "0  Positive  \n",
       "1  Positive  \n",
       "2  Positive  \n",
       "3  Positive  \n",
       "4  Positive  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Apple_tweets[\"Sentiment\"] = Apple_tweets[\"Avg\"].apply(Tweet)\n",
    "\n",
    "Apple_tweets.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:36.577178Z",
     "start_time": "2020-04-16T13:31:36.484426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1181 entries, 0 to 1180\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Tweet       1181 non-null   object \n",
      " 1   Avg         1181 non-null   float64\n",
      " 2   totalwords  1181 non-null   int64  \n",
      " 3   word_count  1181 non-null   int64  \n",
      " 4   char_count  1181 non-null   int64  \n",
      " 5   avg_word    1181 non-null   float64\n",
      " 6   hastags     1181 non-null   int64  \n",
      " 7   hastags_1   1181 non-null   int64  \n",
      " 8   numerics    1181 non-null   int64  \n",
      " 9   upper       1181 non-null   int64  \n",
      " 10  Sentiment   1181 non-null   object \n",
      "dtypes: float64(2), int64(7), object(2)\n",
      "memory usage: 101.6+ KB\n"
     ]
    }
   ],
   "source": [
    "Apple_tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at distribution of different sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:36.798625Z",
     "start_time": "2020-04-16T13:31:36.578174Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "Apple_tweets.Sentiment.value_counts().plot(kind='pie', autopct='%1.0f%%', colors=[\"red\",\"green\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:36.831530Z",
     "start_time": "2020-04-16T13:31:36.799584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1181 entries, 0 to 1180\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Tweet       1181 non-null   object \n",
      " 1   Avg         1181 non-null   float64\n",
      " 2   totalwords  1181 non-null   int64  \n",
      " 3   word_count  1181 non-null   int64  \n",
      " 4   char_count  1181 non-null   int64  \n",
      " 5   avg_word    1181 non-null   float64\n",
      " 6   hastags     1181 non-null   int64  \n",
      " 7   hastags_1   1181 non-null   int64  \n",
      " 8   numerics    1181 non-null   int64  \n",
      " 9   upper       1181 non-null   int64  \n",
      " 10  Sentiment   1181 non-null   object \n",
      "dtypes: float64(2), int64(7), object(2)\n",
      "memory usage: 101.6+ KB\n"
     ]
    }
   ],
   "source": [
    "Apple_tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:36.925285Z",
     "start_time": "2020-04-16T13:31:36.832496Z"
    }
   },
   "outputs": [],
   "source": [
    "processed_features = Apple_tweets.iloc[:, 0].values\n",
    "labels = Apple_tweets.iloc[:, 10].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:37.078835Z",
     "start_time": "2020-04-16T13:31:36.926245Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['say far best customer care service ever received appstore',\n",
       "       'ios 7 fricking smooth beautiful thanxapple', 'love u', ...,\n",
       "       'freaking cows freak', 'hate phone working im going freak',\n",
       "       'agounalakis thats nasty nasty brat'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:37.174614Z",
     "start_time": "2020-04-16T13:31:37.079832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive', 'Positive', 'Positive', ..., 'Negative', 'Negative',\n",
       "       'Negative'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:37.365103Z",
     "start_time": "2020-04-16T13:31:37.175579Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer (max_features=2500, min_df=7, max_df=0.8)\n",
    "processed_features = vectorizer.fit_transform(processed_features).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:37.445855Z",
     "start_time": "2020-04-16T13:31:37.366069Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.8, max_features=2500,\n",
       "                min_df=7, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:37.573551Z",
     "start_time": "2020-04-16T13:31:37.446854Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:37.685213Z",
     "start_time": "2020-04-16T13:31:37.574509Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive', 'Positive', 'Positive', 'Positive', 'Negative',\n",
       "       'Positive', 'Positive', 'Positive', 'Negative', 'Negative',\n",
       "       'Positive', 'Negative', 'Negative', 'Negative', 'Negative',\n",
       "       'Positive', 'Negative', 'Positive', 'Negative', 'Negative',\n",
       "       'Positive', 'Negative', 'Negative', 'Negative', 'Negative',\n",
       "       'Positive', 'Negative', 'Positive', 'Positive', 'Positive',\n",
       "       'Negative', 'Negative', 'Negative', 'Negative', 'Negative',\n",
       "       'Negative', 'Negative', 'Positive', 'Positive', 'Negative',\n",
       "       'Positive', 'Positive', 'Positive', 'Negative', 'Negative',\n",
       "       'Positive', 'Positive', 'Negative', 'Positive', 'Negative',\n",
       "       'Negative', 'Negative', 'Negative', 'Positive', 'Negative',\n",
       "       'Positive', 'Positive', 'Positive', 'Negative', 'Positive',\n",
       "       'Positive', 'Negative', 'Negative', 'Positive', 'Negative',\n",
       "       'Positive', 'Positive', 'Positive', 'Negative', 'Positive',\n",
       "       'Positive', 'Negative', 'Positive', 'Positive', 'Negative',\n",
       "       'Positive', 'Negative', 'Positive', 'Negative', 'Positive',\n",
       "       'Negative', 'Positive', 'Negative', 'Negative', 'Negative',\n",
       "       'Positive', 'Positive', 'Positive', 'Positive', 'Negative',\n",
       "       'Negative', 'Negative', 'Positive', 'Positive', 'Negative',\n",
       "       'Positive', 'Negative', 'Negative', 'Positive', 'Positive',\n",
       "       'Negative', 'Positive', 'Negative', 'Positive', 'Negative',\n",
       "       'Positive', 'Positive', 'Negative', 'Negative', 'Positive',\n",
       "       'Negative', 'Negative', 'Positive', 'Positive', 'Negative',\n",
       "       'Positive', 'Negative', 'Positive', 'Negative', 'Negative',\n",
       "       'Positive', 'Negative', 'Positive', 'Positive', 'Positive',\n",
       "       'Negative', 'Positive', 'Negative', 'Negative', 'Positive',\n",
       "       'Negative', 'Negative', 'Negative', 'Positive', 'Positive',\n",
       "       'Negative', 'Positive', 'Positive', 'Negative', 'Positive',\n",
       "       'Positive', 'Positive', 'Positive', 'Positive', 'Positive',\n",
       "       'Negative', 'Negative', 'Negative', 'Positive', 'Negative',\n",
       "       'Negative', 'Positive', 'Positive', 'Positive', 'Negative',\n",
       "       'Positive', 'Negative', 'Negative', 'Positive', 'Positive',\n",
       "       'Positive', 'Negative', 'Positive', 'Positive', 'Negative',\n",
       "       'Positive', 'Positive', 'Negative', 'Negative', 'Negative',\n",
       "       'Negative', 'Positive', 'Positive', 'Positive', 'Negative',\n",
       "       'Positive', 'Negative', 'Positive', 'Negative', 'Negative',\n",
       "       'Positive', 'Positive', 'Positive', 'Negative', 'Positive',\n",
       "       'Positive', 'Negative', 'Positive', 'Negative', 'Positive',\n",
       "       'Negative', 'Negative', 'Positive', 'Negative', 'Negative',\n",
       "       'Positive', 'Positive', 'Positive', 'Negative', 'Negative',\n",
       "       'Positive', 'Positive', 'Negative', 'Negative', 'Positive',\n",
       "       'Positive', 'Positive', 'Positive', 'Positive', 'Positive',\n",
       "       'Positive', 'Positive', 'Positive', 'Positive', 'Positive',\n",
       "       'Positive', 'Negative', 'Positive', 'Positive', 'Negative',\n",
       "       'Negative', 'Positive', 'Positive', 'Negative', 'Negative',\n",
       "       'Positive', 'Positive', 'Negative', 'Positive', 'Negative',\n",
       "       'Positive', 'Positive', 'Negative', 'Positive', 'Negative',\n",
       "       'Positive', 'Negative', 'Negative', 'Positive', 'Positive',\n",
       "       'Positive', 'Positive', 'Positive', 'Positive', 'Negative',\n",
       "       'Negative', 'Positive', 'Positive', 'Positive', 'Positive',\n",
       "       'Positive', 'Positive', 'Positive', 'Positive', 'Negative',\n",
       "       'Negative', 'Negative', 'Negative', 'Positive', 'Positive',\n",
       "       'Positive', 'Negative', 'Positive', 'Positive', 'Positive',\n",
       "       'Positive', 'Negative', 'Positive', 'Positive', 'Negative',\n",
       "       'Negative', 'Negative', 'Positive', 'Positive', 'Negative',\n",
       "       'Negative', 'Positive', 'Positive', 'Positive', 'Negative',\n",
       "       'Negative', 'Negative', 'Positive', 'Positive', 'Negative',\n",
       "       'Negative', 'Positive', 'Positive', 'Positive', 'Positive',\n",
       "       'Positive', 'Positive', 'Positive', 'Positive', 'Negative',\n",
       "       'Positive', 'Negative', 'Positive', 'Positive', 'Positive',\n",
       "       'Negative', 'Positive', 'Negative', 'Negative', 'Positive',\n",
       "       'Positive', 'Negative', 'Positive', 'Positive', 'Positive',\n",
       "       'Positive', 'Positive', 'Positive', 'Positive', 'Positive',\n",
       "       'Negative', 'Positive', 'Negative', 'Positive', 'Positive',\n",
       "       'Negative', 'Positive', 'Positive', 'Positive', 'Positive',\n",
       "       'Positive', 'Positive', 'Negative', 'Positive', 'Positive',\n",
       "       'Positive', 'Negative', 'Positive', 'Positive', 'Positive',\n",
       "       'Positive', 'Positive', 'Negative', 'Positive', 'Negative',\n",
       "       'Positive', 'Negative', 'Positive', 'Positive', 'Positive',\n",
       "       'Positive', 'Negative', 'Positive', 'Negative', 'Negative',\n",
       "       'Positive', 'Negative', 'Positive', 'Positive', 'Positive',\n",
       "       'Positive', 'Positive', 'Negative', 'Negative', 'Positive',\n",
       "       'Negative', 'Positive', 'Negative', 'Negative', 'Positive',\n",
       "       'Positive', 'Negative', 'Positive', 'Negative', 'Positive',\n",
       "       'Negative', 'Negative', 'Positive', 'Positive', 'Positive',\n",
       "       'Positive', 'Positive', 'Positive', 'Negative', 'Negative',\n",
       "       'Negative', 'Positive', 'Negative', 'Positive', 'Negative',\n",
       "       'Positive', 'Positive', 'Positive', 'Positive', 'Negative',\n",
       "       'Negative', 'Positive', 'Negative', 'Positive', 'Positive',\n",
       "       'Negative', 'Negative', 'Positive', 'Negative', 'Positive',\n",
       "       'Positive', 'Negative', 'Positive', 'Negative', 'Negative',\n",
       "       'Positive', 'Negative', 'Positive', 'Positive', 'Negative',\n",
       "       'Negative', 'Negative', 'Negative', 'Positive', 'Negative',\n",
       "       'Negative', 'Positive', 'Positive', 'Positive', 'Positive',\n",
       "       'Positive', 'Positive', 'Positive', 'Positive', 'Positive',\n",
       "       'Negative', 'Negative', 'Negative', 'Positive', 'Negative',\n",
       "       'Positive', 'Positive', 'Positive', 'Positive', 'Positive',\n",
       "       'Negative', 'Positive', 'Negative', 'Negative', 'Positive',\n",
       "       'Positive', 'Positive', 'Positive', 'Negative', 'Negative',\n",
       "       'Negative', 'Negative', 'Positive', 'Negative', 'Positive',\n",
       "       'Positive', 'Positive', 'Positive', 'Negative', 'Negative',\n",
       "       'Negative', 'Positive', 'Positive', 'Negative', 'Positive',\n",
       "       'Positive', 'Negative', 'Positive', 'Positive', 'Negative',\n",
       "       'Negative', 'Negative', 'Positive', 'Negative', 'Positive',\n",
       "       'Positive', 'Negative', 'Positive', 'Positive', 'Positive',\n",
       "       'Negative', 'Positive', 'Positive', 'Negative', 'Negative',\n",
       "       'Negative', 'Negative', 'Negative', 'Negative', 'Negative',\n",
       "       'Positive', 'Positive', 'Positive', 'Negative', 'Positive',\n",
       "       'Negative', 'Negative', 'Negative', 'Positive', 'Positive',\n",
       "       'Positive', 'Positive', 'Negative', 'Negative', 'Negative',\n",
       "       'Negative', 'Positive', 'Negative', 'Positive', 'Positive',\n",
       "       'Positive', 'Negative', 'Positive', 'Positive', 'Negative',\n",
       "       'Positive', 'Negative', 'Positive', 'Positive', 'Positive',\n",
       "       'Negative', 'Positive', 'Positive', 'Negative', 'Positive',\n",
       "       'Negative', 'Positive', 'Positive', 'Positive', 'Positive',\n",
       "       'Negative', 'Negative', 'Negative', 'Negative', 'Positive',\n",
       "       'Negative', 'Positive', 'Positive', 'Positive', 'Positive',\n",
       "       'Negative', 'Positive', 'Positive', 'Negative', 'Negative',\n",
       "       'Positive', 'Positive', 'Positive', 'Positive', 'Positive',\n",
       "       'Positive', 'Positive', 'Negative', 'Positive', 'Negative',\n",
       "       'Negative', 'Positive', 'Positive', 'Positive', 'Positive',\n",
       "       'Positive', 'Negative', 'Negative', 'Negative', 'Negative',\n",
       "       'Negative', 'Positive', 'Negative', 'Negative', 'Negative',\n",
       "       'Negative', 'Negative', 'Positive', 'Negative', 'Positive',\n",
       "       'Negative', 'Negative', 'Positive', 'Positive', 'Positive',\n",
       "       'Negative', 'Positive', 'Positive', 'Negative', 'Positive',\n",
       "       'Positive', 'Positive', 'Positive', 'Negative', 'Negative',\n",
       "       'Positive', 'Negative', 'Positive', 'Negative', 'Negative',\n",
       "       'Negative', 'Negative', 'Negative', 'Positive', 'Negative',\n",
       "       'Negative', 'Positive', 'Positive', 'Positive', 'Negative',\n",
       "       'Positive', 'Negative', 'Positive', 'Negative', 'Negative',\n",
       "       'Negative', 'Positive', 'Negative', 'Positive', 'Negative',\n",
       "       'Positive', 'Positive', 'Positive', 'Negative', 'Positive',\n",
       "       'Positive', 'Positive', 'Negative', 'Positive', 'Negative',\n",
       "       'Positive', 'Negative', 'Negative', 'Negative', 'Positive',\n",
       "       'Positive', 'Negative', 'Positive', 'Negative', 'Positive',\n",
       "       'Positive', 'Positive', 'Positive', 'Negative', 'Negative',\n",
       "       'Negative', 'Negative', 'Negative', 'Negative', 'Positive',\n",
       "       'Positive', 'Positive', 'Positive', 'Positive', 'Positive',\n",
       "       'Positive', 'Positive', 'Positive', 'Positive', 'Negative',\n",
       "       'Positive', 'Positive', 'Positive', 'Positive', 'Negative',\n",
       "       'Negative', 'Positive', 'Negative', 'Positive', 'Negative',\n",
       "       'Positive', 'Positive', 'Positive', 'Negative', 'Negative',\n",
       "       'Negative', 'Negative', 'Positive', 'Negative', 'Negative',\n",
       "       'Negative', 'Negative', 'Negative', 'Negative', 'Positive',\n",
       "       'Negative', 'Negative', 'Positive', 'Negative', 'Positive',\n",
       "       'Positive', 'Positive', 'Positive', 'Positive', 'Positive',\n",
       "       'Negative', 'Positive', 'Positive', 'Negative', 'Positive',\n",
       "       'Negative', 'Positive', 'Negative', 'Positive', 'Negative',\n",
       "       'Negative', 'Negative', 'Positive', 'Negative', 'Negative',\n",
       "       'Positive', 'Positive', 'Positive', 'Negative', 'Negative',\n",
       "       'Negative', 'Positive', 'Positive', 'Positive', 'Positive',\n",
       "       'Positive', 'Positive', 'Negative', 'Negative', 'Positive',\n",
       "       'Positive', 'Positive', 'Negative', 'Positive', 'Negative',\n",
       "       'Positive', 'Negative', 'Positive', 'Positive', 'Negative',\n",
       "       'Negative', 'Positive', 'Negative', 'Positive', 'Positive',\n",
       "       'Negative', 'Positive', 'Positive', 'Negative', 'Positive',\n",
       "       'Negative', 'Negative', 'Negative', 'Negative', 'Negative',\n",
       "       'Negative', 'Negative', 'Negative', 'Negative', 'Positive',\n",
       "       'Positive', 'Positive', 'Negative', 'Positive', 'Positive',\n",
       "       'Positive', 'Negative', 'Positive', 'Negative', 'Negative',\n",
       "       'Negative', 'Negative', 'Positive', 'Negative', 'Positive',\n",
       "       'Negative', 'Negative', 'Positive', 'Negative', 'Negative',\n",
       "       'Positive', 'Negative', 'Positive', 'Positive', 'Positive',\n",
       "       'Negative', 'Negative', 'Negative', 'Negative', 'Negative',\n",
       "       'Negative', 'Negative', 'Negative', 'Negative', 'Negative',\n",
       "       'Positive', 'Negative', 'Negative', 'Negative', 'Positive',\n",
       "       'Positive', 'Positive', 'Positive', 'Negative', 'Negative',\n",
       "       'Positive', 'Positive', 'Negative', 'Positive', 'Positive',\n",
       "       'Positive', 'Positive', 'Positive', 'Positive', 'Positive',\n",
       "       'Positive', 'Negative', 'Negative', 'Negative', 'Negative',\n",
       "       'Negative', 'Positive', 'Positive', 'Negative', 'Negative',\n",
       "       'Negative', 'Negative', 'Positive', 'Negative', 'Positive',\n",
       "       'Negative', 'Negative', 'Positive', 'Positive', 'Positive',\n",
       "       'Positive', 'Negative', 'Positive', 'Positive', 'Negative',\n",
       "       'Positive', 'Positive', 'Positive', 'Positive', 'Positive',\n",
       "       'Negative', 'Positive', 'Positive', 'Negative', 'Positive',\n",
       "       'Negative', 'Negative', 'Positive', 'Negative', 'Negative',\n",
       "       'Negative', 'Negative', 'Negative', 'Positive', 'Positive',\n",
       "       'Negative', 'Positive', 'Positive', 'Negative', 'Negative',\n",
       "       'Positive', 'Positive', 'Positive', 'Positive', 'Negative',\n",
       "       'Positive', 'Positive', 'Positive', 'Positive', 'Negative',\n",
       "       'Positive', 'Negative', 'Positive', 'Negative', 'Positive',\n",
       "       'Positive', 'Positive', 'Positive', 'Negative', 'Negative',\n",
       "       'Negative', 'Negative', 'Negative', 'Negative', 'Negative',\n",
       "       'Negative', 'Negative', 'Negative', 'Positive', 'Positive',\n",
       "       'Positive', 'Positive', 'Negative', 'Positive', 'Positive',\n",
       "       'Positive', 'Negative', 'Positive', 'Negative', 'Positive',\n",
       "       'Positive', 'Positive', 'Negative', 'Negative', 'Negative',\n",
       "       'Positive', 'Positive', 'Positive', 'Negative', 'Negative',\n",
       "       'Negative', 'Positive', 'Negative', 'Positive', 'Negative',\n",
       "       'Positive', 'Negative', 'Positive', 'Positive', 'Negative',\n",
       "       'Positive', 'Positive', 'Positive', 'Negative', 'Positive',\n",
       "       'Negative', 'Negative', 'Negative', 'Negative', 'Positive',\n",
       "       'Negative', 'Positive', 'Negative', 'Negative', 'Positive',\n",
       "       'Negative', 'Positive', 'Negative', 'Positive', 'Negative',\n",
       "       'Positive', 'Positive', 'Positive', 'Negative', 'Positive',\n",
       "       'Positive', 'Negative', 'Positive', 'Positive', 'Negative',\n",
       "       'Negative', 'Negative', 'Positive', 'Negative'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:38.652626Z",
     "start_time": "2020-04-16T13:31:37.686211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF_model = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "RF_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance Matrix on train data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:38.820178Z",
     "start_time": "2020-04-16T13:31:38.653625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9449152542372882\n",
      "[[380  45]\n",
      " [  7 512]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.98      0.89      0.94       425\n",
      "    Positive       0.92      0.99      0.95       519\n",
      "\n",
      "    accuracy                           0.94       944\n",
      "   macro avg       0.95      0.94      0.94       944\n",
      "weighted avg       0.95      0.94      0.94       944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "y_train_predict = RF_model.predict(X_train)\n",
    "model_score =RF_model.score(X_train, y_train)\n",
    "print(model_score)\n",
    "print(metrics.confusion_matrix(y_train, y_train_predict))\n",
    "print(metrics.classification_report(y_train, y_train_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance Matrix on test data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:38.893022Z",
     "start_time": "2020-04-16T13:31:38.822175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7130801687763713\n",
      "[[75 41]\n",
      " [27 94]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.74      0.65      0.69       116\n",
      "    Positive       0.70      0.78      0.73       121\n",
      "\n",
      "    accuracy                           0.71       237\n",
      "   macro avg       0.72      0.71      0.71       237\n",
      "weighted avg       0.72      0.71      0.71       237\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_predict = RF_model.predict(X_test)\n",
    "model_score = RF_model.score(X_test, y_test)\n",
    "print(model_score)\n",
    "print(metrics.confusion_matrix(y_test, y_test_predict))\n",
    "print(metrics.classification_report(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:39.046609Z",
     "start_time": "2020-04-16T13:31:38.893982Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=1, splitter='best')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "\n",
    "DT_model= tree.DecisionTreeClassifier(random_state=1)\n",
    "DT_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance Matrix on train data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:39.124366Z",
     "start_time": "2020-04-16T13:31:39.047572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9449152542372882\n",
      "[[386  39]\n",
      " [ 13 506]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.97      0.91      0.94       425\n",
      "    Positive       0.93      0.97      0.95       519\n",
      "\n",
      "    accuracy                           0.94       944\n",
      "   macro avg       0.95      0.94      0.94       944\n",
      "weighted avg       0.95      0.94      0.94       944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_predict = DT_model.predict(X_train)\n",
    "model_score = DT_model.score(X_train, y_train)\n",
    "print(model_score)\n",
    "print(metrics.confusion_matrix(y_train, y_train_predict))\n",
    "print(metrics.classification_report(y_train, y_train_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance Matrix on test data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:39.251060Z",
     "start_time": "2020-04-16T13:31:39.125363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6751054852320675\n",
      "[[76 40]\n",
      " [37 84]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.67      0.66      0.66       116\n",
      "    Positive       0.68      0.69      0.69       121\n",
      "\n",
      "    accuracy                           0.68       237\n",
      "   macro avg       0.67      0.67      0.67       237\n",
      "weighted avg       0.68      0.68      0.67       237\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_predict = DT_model.predict(X_test)\n",
    "model_score = DT_model.score(X_test, y_test)\n",
    "print(model_score)\n",
    "print(metrics.confusion_matrix(y_test, y_test_predict))\n",
    "print(metrics.classification_report(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:39.530278Z",
     "start_time": "2020-04-16T13:31:39.252023Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "LDA_model= LinearDiscriminantAnalysis()\n",
    "LDA_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance Matrix on train data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:39.568177Z",
     "start_time": "2020-04-16T13:31:39.532273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.809322033898305\n",
      "[[303 122]\n",
      " [ 58 461]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.84      0.71      0.77       425\n",
      "    Positive       0.79      0.89      0.84       519\n",
      "\n",
      "    accuracy                           0.81       944\n",
      "   macro avg       0.82      0.80      0.80       944\n",
      "weighted avg       0.81      0.81      0.81       944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_predict = LDA_model.predict(X_train)\n",
    "model_score = LDA_model.score(X_train, y_train)\n",
    "print(model_score)\n",
    "print(metrics.confusion_matrix(y_train, y_train_predict))\n",
    "print(metrics.classification_report(y_train, y_train_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance Matrix on test data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:39.650955Z",
     "start_time": "2020-04-16T13:31:39.570173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.70042194092827\n",
      "[[76 40]\n",
      " [31 90]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.71      0.66      0.68       116\n",
      "    Positive       0.69      0.74      0.72       121\n",
      "\n",
      "    accuracy                           0.70       237\n",
      "   macro avg       0.70      0.70      0.70       237\n",
      "weighted avg       0.70      0.70      0.70       237\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_predict = LDA_model.predict(X_test)\n",
    "model_score = LDA_model.score(X_test, y_test)\n",
    "print(model_score)\n",
    "print(metrics.confusion_matrix(y_test, y_test_predict))\n",
    "print(metrics.classification_report(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:39.792577Z",
     "start_time": "2020-04-16T13:31:39.651953Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "KNN_model=KNeighborsClassifier()\n",
    "KNN_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:40.546597Z",
     "start_time": "2020-04-16T13:31:39.793574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7182203389830508\n",
      "[[215 210]\n",
      " [ 56 463]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.79      0.51      0.62       425\n",
      "    Positive       0.69      0.89      0.78       519\n",
      "\n",
      "    accuracy                           0.72       944\n",
      "   macro avg       0.74      0.70      0.70       944\n",
      "weighted avg       0.74      0.72      0.71       944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Performance Matrix on train data set\n",
    "y_train_predict = KNN_model.predict(X_train)\n",
    "model_score = KNN_model.score(X_train, y_train)\n",
    "print(model_score)\n",
    "print(metrics.confusion_matrix(y_train, y_train_predict))\n",
    "print(metrics.classification_report(y_train, y_train_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T13:31:40.730109Z",
     "start_time": "2020-04-16T13:31:40.547561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6075949367088608\n",
      "[[49 67]\n",
      " [26 95]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.65      0.42      0.51       116\n",
      "    Positive       0.59      0.79      0.67       121\n",
      "\n",
      "    accuracy                           0.61       237\n",
      "   macro avg       0.62      0.60      0.59       237\n",
      "weighted avg       0.62      0.61      0.59       237\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Performance Matrix on test data set\n",
    "y_test_predict = KNN_model.predict(X_test)\n",
    "model_score = KNN_model.score(X_test, y_test)\n",
    "print(model_score)\n",
    "print(metrics.confusion_matrix(y_test, y_test_predict))\n",
    "print(metrics.classification_report(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 4 models given above, considering the different between train and test dataset performance parameters, LDA has performed the best. As per the norm difference between the test and train matrix performance parameters can be within +/- 10% for the model to be valid. For a Negative class, for LDA, difference is within the norms. However for Positive class it is bit higher.\n",
    "\n",
    "\n",
    "We can also look at KNN model outout as well wherein difference between train and test is within +/- 10% but the recall value for Negative sentiments is quite low.\n",
    "\n",
    "In terms of sentiments, based on the organization objective, both positive and  negative metrics could be analyzed to define the marketing strategy e.g. based on positive sentiments organization can decide on what needs to be done for a specific product to increase the positive sentiment. Similarly based on the negative keywords organisation can decide what needs to be corrected to decrease the negative sentiment.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pl. note - Model building is an iterative process. Model performance both on the test and train dataset can be improved using feature engineering, feature extraction, hyper parameter tuning (including combination of various parameters).** \n",
    "\n",
    "**Model has to match the business objective and hence various permutation and combinations can be tried on to refine the model**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
